
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression** is the basic learning algorithm for classification problems. It is simple but elegant in its form. This notebook will cover the algorithms of Logistic Regression and build it in Python from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####We start from the binary classifications####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) let's first define the notations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***$m$*** - Number of training examples\n",
    "\n",
    "***$n$*** - Number of features\n",
    "\n",
    "***$X$*** - Features\n",
    "\n",
    "***$y$*** - Target (discrete)\n",
    "\n",
    "***$(X^{(i)}$,$y^{(i)})$*** - $i^{th}$ taining example\n",
    "\n",
    "***$\\theta: (\\theta_1,\\theta_2,...,\\theta_n)^T$*** - model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) The hypothesis of the Logistic Regression is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_\\theta(X) = g(\\theta^TX)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Where\\;\\; g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Here $g(z)$ is called the Sigmoid Function. We will use it for the activation function of neurons later in the Neural Network as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) The log-loss is then used to define the cost function of the Logistic Regression (L2 Regularization Term):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{m}\\Big(y^{(i)}log\\big(h_\\theta(X^{(i)})\\big)+(1-y^{(i)})log\\big(1-h_\\theta(X^{(i)})\\big)\\Big)+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) To minimize cost function with respect to the parameters, we can calculate the gradient (for $j>=1$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}\\Big(h\\big(\\theta(X_{(i)})\\big)-y^{(i)}\\Big)X_j^{(i)}+\\frac{\\lambda}{m}\\theta_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####The following class BinaryLogisticRegression() builds the binary classification model:####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Binary Logistic Regression\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "class BinaryLogisticRegression():\n",
    "\n",
    "    def __init__(self, lamda):\n",
    "\n",
    "        self._lamda = lamda\n",
    "\n",
    "    # Define class variables\n",
    "    _mu = None\n",
    "    _sigma = None\n",
    "    _coef = None\n",
    "\n",
    "    def _feature_norm(self, X):\n",
    "\n",
    "        # Normalize all features to expedite the gradient descent process\n",
    "        mu = np.mean(X, axis=0)\n",
    "        sigma = np.std(X, axis=0)\n",
    "        X_norm = (X - mu) / sigma\n",
    "\n",
    "        return X_norm, mu, sigma\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "\n",
    "        # Formulate sigmoid function\n",
    "        return 1.0 / (1.0 + np.exp(-1.0 * z))\n",
    "\n",
    "    def _cost_calc(self, theta, X, y):\n",
    "\n",
    "        # Formulate cost function\n",
    "        m, n = X.shape\n",
    "        y = y.reshape((m, 1))\n",
    "        theta = theta.reshape((n, 1))\n",
    "        z = X.dot(theta)\n",
    "        h_x = self._sigmoid(z)\n",
    "        J = -1.0 / m * (y.T.dot(np.log(h_x)) + (1 - y).T.dot(np.log(1 - h_x))) \\\n",
    "            + self._lamda / (2.0 * m) * sum(theta[1:]**2)\n",
    "\n",
    "        return J.ravel()\n",
    "\n",
    "    def _gradient_calc(self, theta, X, y):\n",
    "\n",
    "        # Formulate the gradient of the cost function\n",
    "        m, n = X.shape\n",
    "        y = y.reshape((m, 1))\n",
    "        theta = theta.reshape((n, 1))\n",
    "        z = X.dot(theta)\n",
    "        h_x = self._sigmoid(z)\n",
    "        grad = np.zeros((n, 1))\n",
    "        grad[0] = 1.0 / m * sum(h_x - y)\n",
    "        grad[1:] = 1.0 / m * X[:, 1:].T.dot(h_x - y) + float(self._lamda) / m * theta[1:]\n",
    "\n",
    "        return grad.ravel()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Fit the model\n",
    "        m, n = X.shape\n",
    "        X, self._mu, self._sigma = self._feature_norm(X)\n",
    "        X = np.c_[np.ones((m, 1)), X]\n",
    "        theta = np.zeros(X.shape[1])\n",
    "        result = scipy.optimize.minimize(fun=self._cost_calc, x0=theta, args=(X, y),\n",
    "                                         method='BFGS', jac=self._gradient_calc,\n",
    "                                         options={\"maxiter\": 100, \"disp\": False})\n",
    "\n",
    "        self._coef = result.x\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_prob(self, X):\n",
    "\n",
    "        # predict probabilities with the fitted model\n",
    "        m, n = X.shape\n",
    "        X = np.c_[np.ones((m, 1)), (X - self._mu) / self._sigma]\n",
    "        y_prob = self._sigmoid(X.dot(self._coef.reshape((n+1, 1))))\n",
    "\n",
    "        return y_prob.ravel()\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # predict with the fitted model\n",
    "        p = self.predict_prob(X)\n",
    "        y_predict = np.copy(p)\n",
    "        y_predict[p > 0.5] = 1\n",
    "        y_predict[p <= 0.5] = 0\n",
    "\n",
    "        return y_predict.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Now let's focus on multi-class classification####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the **\"one-versus-all\"** scheme to achive the goal. To be specific, for each class, we code it as the positive class (1), while the rest of classes are coded as the negative class (0). We then apply Binary Logistic Regression and predict the probabilities of being positive for each class against all others. The class with the highest probability of being positive will be chosen as the final prediction of the classification task."
   ]
  },