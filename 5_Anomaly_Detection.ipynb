
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anomaly Detection** is an interesting modeling algorithm that can be used to identify rare cases (such as fraud detection). It is normally considered as an unsupervised learning technique. From personal experiecne, I've also appied it for skewed binary classifications and achieved reasonable reasults. Let's now get to the algorithm itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial steps of anomaly detection can be summarized as:\n",
    "\n",
    "- Find feature vector $X_1, X_2, ..., X_n$($n$ features) that might be considered to be relevant with the anomaly cases\n",
    "\n",
    "- Fit parameters  $(\\mu_1,\\sigma_1^2), (\\mu_2,\\sigma_2^2), ... (\\mu_n,\\sigma_n^2)$ for Gaussian distributions of all the features. If we have $m$ examples, the fitted parameter can be expressed as:\n",
    "\n",
    "$$\\mu_j=\\frac{1}{m}\\sum_{i=1}^{m}X_j^{(i)}$$\n",
    "\n",
    "$$\\sigma_j^2=\\frac{1}{m}\\sum_{i=1}^{m}(X_j^{(i)}-\\mu_j)^2$$\n",
    "\n",
    "- To test a new example $x$, we can compute $p(x)$:\n",
    "\n",
    "$$p(x)=\\prod_{j=1}^{n}\\frac{1}{\\sqrt{2\\pi}\\sigma_j}exp\\big(-\\frac{(X_j-\\mu_j)^2}{2\\sigma_j^2}\\big)$$\n",
    "\n",
    "- If $p(x)<\\epsilon$, we would consider it as an anomaly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The tricky part is to find the proper value for $\\epsilon$. We can do this by borrowing the idea of cross-validation from supervised learning. That is, if we have a data set containing $m1$ normal examples, and $m2$ abnormal cases. We then divide $m1$ good examples into three parts: say $m11, m12, m13$. We also need to split the $m2$ bad examples into two pieces - $m21, m22$.\n",
    "\n",
    "To learn the parameters of the Gaussian Distributions, we can use all good $m11$ examples.\n",
    "\n",
    "To learn the values of the $\\epsilon$, we can put $m12$ good examples and $m21$ bad examples together, and coded them as 0 and 1. Then we loop through a list of possible $\\epsilon$ values. For each $\\epsilon$, we calculate the corresponding performance of this anomaly detection algorithm (we will use f1-score as evaluation metric to deal with the imbalanced class distribution).\n",
    "\n",
    "In the end, we are left with $m13$ good cases and $m22$ bad cases for test purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####If features are highly correlated, we can apply multivariate Gaussian Distribution instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is still the same. The fitting of distribution parameters and the calcualtion of $p(x)$ are slightly changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean vector and the variance-covariance matrix can be expressed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mu=\\frac{1}{m}\\sum_{i=1}^{m}X^{(i)}$$\n",
    "\n",
    "$$\\Sigma=\\frac{1}{m}\\sum_{i=1}^{m}(X^{(i)}-\\mu)(X^{(i)}-\\mu)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And $p(x)$ can be calcuated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(x;\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}exp\\Bigg(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\Bigg)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####The following class AnomalyDetection() covers both non-multivariate and multivariate Gaussian models, and implement both ideas from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "class AnomalyDetection():\n",
    "\n",
    "    def __init__(self, multi_variate=False):\n",
    "\n",
    "        # if multi_variate is True, we will use multivariate Gaussian distribution\n",
    "        # to estimate the probabilities\n",
    "        self.multi_variate = multi_variate\n",
    "        self.mu = None\n",
    "        self.sigma2 = None\n",
    "        self.best_epsilon = 0\n",
    "        self.best_f1_score = 0\n",
    "\n",
    "    def _fit_gaussian(self, X_train):\n",
    "\n",
    "        # fit the parameters of the Gaussian Distribution\n",
    "\n",
    "        # if not using the multivariate Gaussian Distribution, we will estimate\n",
    "        # mu and sigma for each single feature distribution separately\n",
    "        if self.multi_variate is False:\n",
    "            self.mu = np.mean(X_train, axis=0)\n",
    "            self.sigma2 = np.var(X_train, axis=0)\n",
    "\n",
    "        # if using the multivariate Gaussian Distribution, we estimate the vector\n",
    "        # of mu and variance/covariance matrix of sigma\n",
    "        else:\n",
    "            m = X_train.shape[0]\n",
    "            self.mu = np.mean(X_train, axis=0)\n",
    "            self.sigma2 = 1.0 / m * (X_train - self.mu).T.dot(X_train - self.mu)\n",
    "\n",
    "    def _prob_calc(self, X):\n",
    "\n",
    "        # helper function to calculate the probability of each instance\n",
    "        # in the cross-validation set\n",
    "        if self.multi_variate is False:\n",
    "            p = np.prod(np.exp(-(X - self.mu) ** 2 / (2.0 * self.sigma2))\n",
    "                        / np.sqrt(2.0 * math.pi * self.sigma2), axis=1)\n",
    "\n",
    "        else:\n",
    "            n = X.shape[1]\n",
    "            p = 1.0 / ((2 * math.pi) ** (n / 2.0) * (np.linalg.det(self.sigma2) ** 0.5)) \\\n",
    "                * np.diag(np.exp(-0.5 * ((X - self.mu).dot(np.linalg.inv(self.sigma2))) \\\n",
    "                                 .dot((X - self.mu).T)))\n",
    "\n",
    "        return p\n",
    "\n",
    "    def _fit_epsilon(self, X_val, y_val):\n",
    "\n",
    "        # this is the second step of model fitting\n",