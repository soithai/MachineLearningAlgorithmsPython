{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is widely used in Machine Learning pipelines as a means to compress data or help visualization. This notebook aims to walk through the basic idea of the PCA and build the algorithm from scratch in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving directly into the PCA, let's first talk about several import concepts - the **\"eigenvectors & eigenvalues\"** and **\"Singular Value Decomposition (SVD)\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **eigenvector** of a square matrix is a column vector that satisfies:\n",
    "\n",
    "$$Av=\\lambda v$$\n",
    "\n",
    "Where A is a $[n\\times n]$ square matrix, v is a $[n\\times 1]$ **eigenvector**, and $\\lambda$ is a scalar value which is also known as the **eigenvalue**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If A is both a square and symmetric matrix (like a typical variance-covariance matrix), then we can write A as:\n",
    "\n",
    "$$A=U\\Sigma U^T$$\n",
    "\n",
    "Here columns of matrix U are eigenvectors of matrix A; and $\\Sigma$ is a diaonal matrix containing the corresponding eigenvalues. \n",
    "\n",
    "This is also a special case of the well-known theorem **\"Singular Value Decomposition\" (SVD)**, where a rectangular matrix M can be expressed as:\n",
    "\n",
    "$$M=U\\Sigma V^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####With SVD, we can calcuate the eigenvectors and eigenvalues of a square & symmetric matrix. This will be the key to solve the PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the PCA is to find a lower dimension surface to maxmize total variance of the projection, or in other means, to minimize the projection error. The entire algorithm can be summarized as the following:\n",
    "\n",
    "1) Given a data matrix **$X$** with **$m$** rows (number of records) and **$n$** columns (number of dimensions), we should first substract the column mean for each dimension.\n",
    "\n",
    "2) Then we can calculate the variance-covariance matrix using the equation (X here alre