{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is widely used in Machine Learning pipelines as a means to compress data or help visualization. This notebook aims to walk through the basic idea of the PCA and build the algorithm from scratch in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving directly into the PCA, let's first talk about several import concepts - the **\"eigenvectors & eigenvalues\"** and **\"Singular Value Decomposition (SVD)\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **eigenvector** of a square matrix is a column vector that satisfies:\n",
    "\n",
    "$$Av=\\lambda v$$\n",
    "\n",
    "Where A is a $[n\\times n]$ square matrix, v is a $[n\\times 1]$ **eigenvector**, and $\\lambda$ is a scalar value which is also known as the **eigenvalue**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If A is both a square and symmetric matrix (like a typical variance-covariance matrix), then we can write A as:\n",
    "\n",
    "$$A=U\\Sigma U^T$$\n",
    "\n",
    "Here columns of matrix U are eigenvectors of matrix A; and $\\Sigma$ is a diaonal matrix containing the corresponding eigenvalues. \n",
    "\n",
    "This is also a special case of the well-known theorem **\"Singular Value Decomposition\" (SVD)**, where a rectangular matrix M can be expressed as:\n",
    "\n",
    "$$M=U\\Sigma V^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####With SVD, we can calcuate the eigenvectors and eigenvalues of a square & symmetric matrix. This will be the key to solve the PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the PCA is to find a lower dimension surface to maxmize total variance of the projection, or in other means, to minimize the projection error. The entire algorithm can be summarized as the following:\n",
    "\n",
    "1) Given a data matrix **$X$** with **$m$** rows (number of records) and **$n$** columns (number of dimensions), we should first substract the column mean for each dimension.\n",
    "\n",
    "2) Then we can calculate the variance-covariance matrix using the equation (X here already has zero mean for each column from step 1):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$cov=\\frac{1}{m}X^TX$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) We can then use SVD to compute the eigenvectors and corresponding eigenvalues of the above covariance matrix \"$cov$\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$cov=U\\Sigma U^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) If our target dimension is $p$ ($p<n$), then we will select the first $p$ columns of the $U$ matrix and get matrix $U_{reduce}$.\n",
    "\n",
    "5) To get the compressed data set, we can do the transformation as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X_{reduce}=XU_{reduce}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) To appoximate the original data set given the compressed data, we can use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X=X_{reduce}U_{reduce}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this is true because $U_{reduce}^{-1}=U_{reduce}^T$ (in this case, all the eigenvectors are unit vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####In practice, it is also important to choose the proper number of principal components. For data compression, we want to retain as much variation in the original data while reducing the dimension. Luckily, with SVD, we can get a estimate of the retained variation by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\%\\ of\\ variance\\ retained = \\frac{\\sum_{i=1}^{p}S_{ii}}{\\sum_{i=1}^{n}S_{ii}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $S_{ii}$ is the $ith$ diagonal element of the $\\Sigma$ matrix, $p$ is the number of reduced dimension, and $n$ is the dimension of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####For data visulization purposes, we usually choose 2 or 3 dimensions to plot the compressed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####The following class PCA() implements the idea of principal component analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PCA():\n",
    "\n",
    "    def __init__(self, num_components):\n",
    "\n",
    "        self.num_components = num_components\n",
    "        self.U = None\n",
    "        self.S = None\n",
    "\n",
    "    def fit(self, X):\n",
    "\n",
    "        # perform pca\n",
    "        m = X.shape[0]\n",
    "        X_mean = np.mean(X, axis=0)\n",
    "        X -= X_mean\n",
    "        cov = X.T.dot(X) * 1.0 / m\n",
    "        self.U, self.S, _ = np.linalg.svd(cov)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def project(self, X):\n",
    "\n",
    "        # project data based on reduced dimension\n",
    "        U_reduce = self.U[:, :self.num_components]\n",
    "        X_reduce =  X.dot(U_reduce)\n",
    "\n",
    "        return X_reduce\n",
    "\n",
    "    def inverse(self, X_reduce):\n",
    "\n",
    "        # recover the original data based on the reduced form\n",
    "        U_reduce = self.U[:, :self.num_components]\n",
    "        X = X_reduce.dot(U_reduce.T)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def explained_variance(self):\n",
    "\n",
    "        # print the ratio of explained variance with the pca\n",
    "        explained = np.sum(self.S[:self.num_components])\n",
    "        total = np.sum(self.S)\n",
    "\n",
    "        return explained * 1.0 / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Now we can use a demo data set to show dimensionality reduction and data visualization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Iris Data set as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150L, 4L)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = lo