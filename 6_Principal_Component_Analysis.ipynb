{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is widely used in Machine Learning pipelines as a means to compress data or help visualization. This notebook aims to walk through the basic idea of the PCA and build the algorithm from scratch in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving directly into the PCA, let's first talk about several import concepts - the **\"eigenvectors & eigenvalues\"** and **\"Singular Value Decomposition (SVD)\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **eigenvector** of a square matrix is a column vector that satisfies:\n",
    "\n",
    "$$Av=\\lambda v$$\n",
    "\n",
    "Where A is a $[n\\times n]$ square matrix, v is a $[n\\times 1]$ **eigenvector**, and $\\lambda$ is a scalar value which is also known as the **eigenvalue**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If A is both a square and symmetric matrix (like a typical variance-covariance matrix), then we can write A as:\n",
    "\n",
    "$$A=U\\Sigma U^T$$\n",
    "\n",
    "Here columns of matrix U are eigenvectors of matrix A; and $\\Sigma$ is a diaonal matrix containing the corresponding eigenvalues. \n",
    "\n",
    "This is also a special case of the well-known theorem **\"Singular Value Decomposition\" (SVD)**, where a rectangular matrix M can be expressed as:\n",
    "\n",
    "$$M=U\\Sigma V^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####With SVD, we can calcuate the eigenvectors and eigenvalues of a square & symmetric matrix. This will be the key to solve the PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the PCA is to find a lower dimension surface to maxmize total variance of the projection, or in other means, to minimize the projection error. The entire algorithm can be summarized as the following:\n",
    "\n",
    "1) Given a data matrix **$X$** with **$m$** rows (number of records) and **$n$** columns (number of dimensions), we should first substract the column mean for each dimension.\n",
    "\n",
    "2) Then we can calculate the variance-covariance matrix using the equation (X here already has zero mean for each column from step 1):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$cov=\\frac{1}{m}X^TX$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) We can then use SVD to compute the eigenvectors and corresponding eigenvalues of the above covariance matrix \"$cov$\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$cov=U\\Sigma U^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) If our target dimension is $p$ ($p<n$), then we will select the first $p$ columns of the $U$ matrix and get matrix $U_{reduce}$.\n",
    "\n",
    "5) To get the compressed data set, we can do the transformation as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X_{reduce}=XU_{reduce}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) To appoximate the original data set given the compressed data, we can use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X=X_{reduce}U_{reduce}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this is true because $U_{reduce}^{-1}=U_{reduce}^T$ (in this case, all the eigenvectors are unit vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####In practice, it is also important to choose the proper number of principal components. For data compression, we want to retain as much variation in the original data while reducing the dimension. Luckily, with SVD, we can get a estimate of the retained variation by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\%\\ of\\ variance\\ retained = \\frac{\\sum_{i=1}^{p}S_{ii}}{\\sum_{i=1}^{n}S_{ii}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $S_{ii}$ is the $ith$ diagonal element of the $\\Sigma$ matrix, $p$ is the number of reduced dimension, and $n$ is the dimension of the origi