{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is widely used in Machine Learning pipelines as a means to compress data or help visualization. This notebook aims to walk through the basic idea of the PCA and build the algorithm from scratch in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving directly into the PCA, let's first talk about several import concepts - the **\"eigenvectors & eigenvalues\"** and **\"Singular Value Decomposition (SVD)\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **eigenvector** of a square matrix is a column vector that satisfies:\n",
    "\n",
    "$$Av=\\lambda v$$\n",
    "\n",
    "Where A is a $[n\\times n]$ square matrix, v is a $[n\\times 1]$ **eigenvector**, and $\\lambda$ is a scalar value which is also known as the **eigenvalue**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If A is both a square and symmetric matrix (like a typical variance-covariance matrix), then we can write A as:\n",
    "\n",
    "$$A=U\\Sigma U^T$$\n",
    "\n",
    "Here columns of matrix U are eigenvectors of matrix A; and $\\Sigma$ is a diaonal matrix containing the corresponding eigenvalues. \n",
    "